-- =========================================================
-- Drop tables (for clean init)
-- =========================================================
DROP TABLE IF EXISTS chat CASCADE;
DROP TABLE IF EXISTS group_view CASCADE;
DROP TABLE IF EXISTS "groups" CASCADE;
DROP TABLE IF EXISTS "key" CASCADE;
DROP TABLE IF EXISTS model CASCADE;
DROP TABLE IF EXISTS model_catalog CASCADE;
DROP TABLE IF EXISTS provider_catalog CASCADE;
DROP TABLE IF EXISTS room CASCADE;
DROP TABLE IF EXISTS "member" CASCADE;

-- =========================================================
-- TABLE: member
-- =========================================================
CREATE TABLE public."member" (
    created_at    timestamp(6) NOT NULL,
    member_uid    int8 GENERATED BY DEFAULT AS IDENTITY (
                      INCREMENT BY 1
                      MINVALUE 1
                      MAXVALUE 9223372036854775807
                      START 1
                      CACHE 1
                      NO CYCLE
                  ) NOT NULL,
    updated_at    timestamp(6) NULL,
    email         varchar(255) NOT NULL,
    "name"        varchar(255) NOT NULL,
    "password"    varchar(255) NOT NULL,
    CONSTRAINT member_pkey PRIMARY KEY (member_uid)
);

-- =========================================================
-- TABLE: provider_catalog
-- =========================================================
CREATE TABLE public.provider_catalog (
    is_active            bool NULL,
    created_at           timestamp(6) NOT NULL,
    provider_catalog_uid int8 GENERATED BY DEFAULT AS IDENTITY (
                              INCREMENT BY 1
                              MINVALUE 1
                              MAXVALUE 9223372036854775807
                              START 1
                              CACHE 1
                              NO CYCLE
                          ) NOT NULL,
    updated_at           timestamp(6) NULL,
    code                 varchar(255) NOT NULL,
    CONSTRAINT provider_catalog_code_key UNIQUE (code),
    CONSTRAINT provider_catalog_pkey PRIMARY KEY (provider_catalog_uid)
);

-- =========================================================
-- TABLE: model_catalog
-- =========================================================
CREATE TABLE public.model_catalog (
    is_active            bool NOT NULL,
    created_at           timestamp(6) NOT NULL,
    model_catalog_uid    int8 GENERATED BY DEFAULT AS IDENTITY (
                              INCREMENT BY 1
                              MINVALUE 1
                              MAXVALUE 9223372036854775807
                              START 1
                              CACHE 1
                              NO CYCLE
                          ) NOT NULL,
    provider_catalog_uid int8 NOT NULL,
    updated_at           timestamp(6) NULL,
    code                 varchar(255) NOT NULL,
    "name"               varchar(255) NOT NULL,
    CONSTRAINT model_catalog_pkey PRIMARY KEY (model_catalog_uid),
    CONSTRAINT model_catalog_provider_catalog_uid_code_key UNIQUE (provider_catalog_uid, code)
);

-- =========================================================
-- TABLE: key
-- =========================================================
CREATE TABLE public."key" (
    expiration_at        date NOT NULL,
    is_active            bool NOT NULL,
    token_usage          int4 NOT NULL,
    created_at           timestamp(6) NOT NULL,
    key_uid              int8 GENERATED BY DEFAULT AS IDENTITY (
                              INCREMENT BY 1
                              MINVALUE 1
                              MAXVALUE 9223372036854775807
                              START 1
                              CACHE 1
                              NO CYCLE
                          ) NOT NULL,
    member_uid           int8 NOT NULL,
    provider_catalog_uid int8 NOT NULL,
    updated_at           timestamp(6) NULL,
    encrypted_key        text NOT NULL,
    CONSTRAINT key_pkey PRIMARY KEY (key_uid)
);

-- =========================================================
-- TABLE: model
-- =========================================================
CREATE TABLE public.model (
    is_default       bool NOT NULL,
    created_at       timestamp(6) NOT NULL,
    member_uid       int8 NOT NULL,
    model_catalog_uid int8 NOT NULL,
    model_uid        int8 GENERATED BY DEFAULT AS IDENTITY (
                         INCREMENT BY 1
                         MINVALUE 1
                         MAXVALUE 9223372036854775807
                         START 1
                         CACHE 1
                         NO CYCLE
                     ) NOT NULL,
    updated_at       timestamp(6) NULL,
    CONSTRAINT model_pkey PRIMARY KEY (model_uid)
);

-- =========================================================
-- TABLE: room
-- =========================================================
CREATE TABLE public.room (
    created_at  timestamp(6) NOT NULL,
    owner_id    int8 NOT NULL,
    room_uid    int8 GENERATED BY DEFAULT AS IDENTITY (
                    INCREMENT BY 1
                    MINVALUE 1
                    MAXVALUE 9223372036854775807
                    START 1
                    CACHE 1
                    NO CYCLE
                ) NOT NULL,
    updated_at  timestamp(6) NULL,
    branch_view json NOT NULL,
    chat_info   json NOT NULL,
    "name"      varchar(255) NOT NULL,
    CONSTRAINT room_pkey PRIMARY KEY (room_uid)
);

-- =========================================================
-- TABLE: groups
-- =========================================================
CREATE TABLE public."groups" (
    created_at timestamp(6) NOT NULL,
    group_uid  int8 GENERATED BY DEFAULT AS IDENTITY (
                   INCREMENT BY 1
                   MINVALUE 1
                   MAXVALUE 9223372036854775807
                   START 1
                   CACHE 1
                   NO CYCLE
               ) NOT NULL,
    owner_id   int8 NOT NULL,
    updated_at timestamp(6) NULL,
    keywords   text NULL,
    "name"     text NOT NULL,
    summary    text NULL,
    CONSTRAINT groups_pkey PRIMARY KEY (group_uid)
);

-- =========================================================
-- TABLE: group_view
-- =========================================================
CREATE TABLE public.group_view (
    created_at     timestamp(6) NOT NULL,
    group_view_uid int8 GENERATED BY DEFAULT AS IDENTITY (
                        INCREMENT BY 1
                        MINVALUE 1
                        MAXVALUE 9223372036854775807
                        START 1
                        CACHE 1
                        NO CYCLE
                    ) NOT NULL,
    member_id      int8 NOT NULL,
    updated_at     timestamp(6) NULL,
    "content"      text NULL,
    CONSTRAINT group_view_pkey PRIMARY KEY (group_view_uid)
);

-- =========================================================
-- TABLE: chat
-- =========================================================
CREATE TABLE public.chat (
    answered_at      timestamp(6) NULL,
    chat_uid         int8 GENERATED BY DEFAULT AS IDENTITY (
                          INCREMENT BY 1
                          MINVALUE 1
                          MAXVALUE 9223372036854775807
                          START 1
                          CACHE 1
                          NO CYCLE
                      ) NOT NULL,
    created_at       timestamp(6) NOT NULL,
    group_id         int8 NULL,
    model_catalog_uid int8 NULL,
    origin_id        int8 NULL,
    room_id          int8 NULL,
    updated_at       timestamp(6) NULL,
    answer           text NULL,
    is_chat          varchar(255) NOT NULL,
    keywords         text NULL,
    question         text NULL,
    search_content   text NULL,
    status           varchar(255) NOT NULL,
    summary          text NULL,
    CONSTRAINT chat_is_chat_check CHECK (
        (is_chat)::text = ANY (
            (ARRAY['CHAT'::character varying, 'GROUP'::character varying])::text[]
        )
    ),
    CONSTRAINT chat_pkey PRIMARY KEY (chat_uid),
    CONSTRAINT chat_status_check CHECK (
        (status)::text = ANY (
            (ARRAY[
                'QUESTION'::character varying,
                'ANSWER'::character varying,
                'PENDING'::character varying,
                'SUMMARY_KEYWORDS'::character varying
            ])::text[]
        )
    )
);

-- =========================================================
-- FOREIGN KEYS
-- =========================================================

-- key
ALTER TABLE public."key"
    ADD CONSTRAINT fk6fcetw0c19j7bnf1ardvxd26f
        FOREIGN KEY (member_uid) REFERENCES public."member"(member_uid);

ALTER TABLE public."key"
    ADD CONSTRAINT fksenhg834jsdjmp55mkx1tow1a
        FOREIGN KEY (provider_catalog_uid) REFERENCES public.provider_catalog(provider_catalog_uid);

-- model_catalog
ALTER TABLE public.model_catalog
    ADD CONSTRAINT fk9f8v6pjds2q89rxceos5uixjk
        FOREIGN KEY (provider_catalog_uid) REFERENCES public.provider_catalog(provider_catalog_uid);

-- model
ALTER TABLE public.model
    ADD CONSTRAINT fkg0hfffmfuaj5ofwaq378u5ub0
        FOREIGN KEY (member_uid) REFERENCES public."member"(member_uid);

ALTER TABLE public.model
    ADD CONSTRAINT fkh8xglhlslsn14fdt092j3ek3a
        FOREIGN KEY (model_catalog_uid) REFERENCES public.model_catalog(model_catalog_uid);

-- room
ALTER TABLE public.room
    ADD CONSTRAINT fkesostdj4wdhfxom08clfqxtcf
        FOREIGN KEY (owner_id) REFERENCES public."member"(member_uid);

-- groups
ALTER TABLE public."groups"
    ADD CONSTRAINT fkd7y6agdaq9skoclcua8jt0mg7
        FOREIGN KEY (owner_id) REFERENCES public."member"(member_uid);

-- group_view
ALTER TABLE public.group_view
    ADD CONSTRAINT fkmp9ve5byflv0agr9qfntak915
        FOREIGN KEY (member_id) REFERENCES public."member"(member_uid);

-- chat
ALTER TABLE public.chat
    ADD CONSTRAINT fkasl6ie6lwhuyqr5fyxvsig2sm
        FOREIGN KEY (group_id) REFERENCES public."groups"(group_uid);

ALTER TABLE public.chat
    ADD CONSTRAINT fkc0bssq38xilgoxsjdjdg2xaxm
        FOREIGN KEY (model_catalog_uid) REFERENCES public.model_catalog(model_catalog_uid);

ALTER TABLE public.chat
    ADD CONSTRAINT fkm38tfuuhbqvc3jrrat6q4k01j
        FOREIGN KEY (room_id) REFERENCES public.room(room_uid);

-- =========================================================
-- INDEXES
-- =========================================================

CREATE INDEX idx_chat_room_status_type_created
    ON public.chat USING btree (room_id, status, is_chat, created_at DESC);

-- ========================================================
-- provider_catalog 더미 데이터
-- ========================================================

INSERT INTO public.provider_catalog
    (created_at, updated_at, code, is_active)
VALUES
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'openai',     TRUE),
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'anthropic', TRUE),
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'gemini',    TRUE);

-- =========================================================
-- model_catalog 더미 데이터
-- =========================================================

INSERT INTO public.model_catalog
    (created_at, updated_at, code, is_active, "name", provider_catalog_uid)
VALUES
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'gpt-4o',                      TRUE, 'GPT-4o',                 1),
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'gpt-4o-mini',                 TRUE, 'GPT-4o-Mini',            1),
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'gpt-5-mini',                  TRUE, 'GPT-5-Mini',             1),
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'gpt-5',                       TRUE, 'GPT-5',                  1),
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'gpt-5-nano',                  TRUE, 'GPT-5-Nano',             1),
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'claude-sonnet-4-20250514',    TRUE, 'Claude-Sonnet-4',        2),
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'claude-opus-4-1-20250805',    TRUE, 'Claude-Opus-4-1',        2),
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'claude-3-7-sonnet-latest',    TRUE, 'Claude-3-7-Sonnet',      2),
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'claude-3-5-haiku-latest',     TRUE, 'Claude-3-5-Haiku',       2),
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'gemini-2.5-pro',              TRUE, 'Gemini-2.5-pro',         3),
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'gemini-2.5-flash',            TRUE, 'Gemini-2.5-Flash',       3),
    ('2025-11-19 09:22:38.559077', '2025-11-19 09:22:38.559077', 'gemini-2.5-flash-lite',       TRUE, 'Gemini-2.5-Flash-Lite',  3);

-- =========================================================
-- member 더미 데이터
-- =========================================================

INSERT INTO public."member"
    (created_at, updated_at, email, "name", "password")
VALUES
    ('2025-11-19 09:18:08.991141', '2025-11-19 09:18:08.991141',
     'rladkgus01@gmail.com', '염소',
     '$2a$10$0/0HI3gJGGF4OMGmSp9wjujRfPbuB.2tZZ1lxDhukkl3bQWGHVucm'),

    ('2025-11-19 09:18:21.289109', '2025-11-19 09:18:21.289109',
     'rladkgus012@gmail.com', '흑염소',
     '$2a$10$S6ywPo0PNvP5cUCrZmTkMO1NPVS585VVwcA/N7D3Cq1Jj3i1mLEXO'),

    ('2025-11-19 09:17:42.737713', '2025-11-19 09:17:42.737713',
     'test@example.com', 'test_user',
     '$2a$10$5Ngl9eweoZbZYVV4YNLB3OHm1pEY0fOzD.lZ/0BtQ3j0VBzbu2eAq');

-- =========================================================
-- key 더미 데이터
-- =========================================================

INSERT INTO public."key"
    (expiration_at, is_active, token_usage, created_at,
     member_uid, provider_catalog_uid, updated_at, encrypted_key)
VALUES
    (
        '2026-12-31',
        TRUE,
        0,
        '2025-11-19 09:30:00',
        1,  
        1,  
        '2025-11-19 09:30:00',
        'ENC_OPENAI_KEY_FOR_YEOMSO'
    ),
    (
        '2026-12-31',
        TRUE,
        5,
        '2025-11-19 09:30:10',
        2,  
        2, 
        '2025-11-19 09:30:10',
        'ENC_ANTHROPIC_KEY_FOR_BLACK_YEOMSO'
    ),
    (
        '2026-12-31',
        TRUE,
        2,
        '2025-11-19 09:30:20',
        3,  
        3,  
        '2025-11-19 09:30:20',
        'ENC_GEMINI_KEY_FOR_TEST_USER'
    );

-- =========================================================
-- model 더미 데이터
-- =========================================================

INSERT INTO public.model
    (is_default, created_at, member_uid, model_catalog_uid, updated_at)
VALUES
    (
        TRUE,
        '2025-11-19 09:31:00',
        1,   
        3,  
        '2025-11-19 09:31:00'
    ),
    (
        FALSE,
        '2025-11-19 09:31:10',
        1,  
        1, 
        '2025-11-19 09:31:10'
    ),
    (
        TRUE,
        '2025-11-19 09:31:20',
        2,   
        8,   
        '2025-11-19 09:31:20'
    ),
    (
        TRUE,
        '2025-11-19 09:31:30',
        3, 
        11,
        '2025-11-19 09:31:30'
    );

-- =========================================================
-- room 더미 데이터
-- =========================================================

INSERT INTO public.room
    (room_uid, created_at, owner_id, updated_at, branch_view, chat_info, "name")
VALUES
    (
        1,
        '2025-11-19 09:32:00',
        1,  
        '2025-11-19 09:32:00',
        $$ {"nodes": [], "edges": []} $$::json,
        $$ {"rootChatUid": null, "chatCount": 0} $$::json,
        '염소의 실험 방'
    ),
    (
        2,
        '2025-11-19 09:32:10',
        2,  
        '2025-11-19 09:32:10',
        $$ {"nodes": [{"id": "1"}], "edges": []} $$::json,
        $$ {"rootChatUid": 1, "chatCount": 2} $$::json,
        '흑염소의 검색 튜닝 방'
    ),
    (
        3,
        '2025-11-19 09:32:20',
        3,  
        '2025-11-19 09:32:20',
        $$ {"nodes": [{"id": "1"}, {"id": "2"}], "edges": [{"source": "1","target": "2"}]} $$::json,
        $$ {"rootChatUid": 3, "chatCount": 2} $$::json,
        'test_user의 테스트 방'
    );

-- =========================================================
-- groups 더미 데이터
-- =========================================================

INSERT INTO public."groups"
    (group_uid, created_at, owner_id, updated_at, keywords, "name", summary)
VALUES
    (
        1,
        '2025-11-19 09:33:00',
        1,  -- 염소
        '2025-11-19 09:33:00',
        '요약,키워드,pg_bigm',
        '요약/검색 실험 그룹',
        '요약과 키워드, 검색 관련 채팅을 모아둔 그룹입니다.'
    ),
    (
        2,
        '2025-11-19 09:33:10',
        2,  -- 흑염소
        '2025-11-19 09:33:10',
        '성능,인덱스,튜닝',
        '성능 튜닝 그룹',
        '쿼리, 인덱스, 캐싱 실험 결과를 모아둔 그룹입니다.'
    );

-- =========================================================
-- group_view 더미 데이터
-- =========================================================

INSERT INTO public.group_view
    (group_view_uid, created_at, member_id, updated_at, "content")
VALUES
    (
        1,
        '2025-11-19 09:34:00',
        1,  
        '2025-11-19 09:34:00',
        '["요약/검색 실험 그룹", "성능 튜닝 그룹"]'
    ),
    (
        2,
        '2025-11-19 09:34:10',
        2,  
        '2025-11-19 09:34:10',
        '["성능 튜닝 그룹"]'
    ),
    (
        3,
        '2025-11-19 09:34:20',
        3, 
        '2025-11-19 09:34:20',
        '["요약/검색 실험 그룹"]'
    );

-- =========================================================
-- chat 더미 데이터
-- =========================================================

INSERT INTO public.chat
    (answered_at, created_at, group_id, model_catalog_uid, origin_id,
     room_id, updated_at, answer, is_chat, keywords, question,
     search_content, status, summary)
VALUES
    (
        '2025-11-19 09:35:20',
        '2025-11-19 09:35:20',
        NULL,   
        3,   
        NULL,
        1, 
        '2025-11-19 09:35:20',
        'pg_bigm은 ILIKE와 함께 사용할 수 있고, GIN 인덱스를 통해 다중 키워드 AND 검색도 가능하다. 다만 쿼리 플랜을 확인해서 seq scan을 피하도록 주의해야 한다.',
        'CHAT',
        'pg_bigm,GIN,ILIKE,다중 키워드,쿼리 플랜',
        'pg_bigm으로 다중 키워드 검색 최적화하려면 어떻게 해야 해?',
        'pg_bigm으로 다중 키워드 검색 최적화하려면 어떻게 해야 해? '
        'pg_bigm은 ILIKE와 함께 사용할 수 있고, GIN 인덱스를 통해 다중 키워드 AND 검색도 가능하다. 다만 쿼리 플랜을 확인해서 seq scan을 피하도록 주의해야 한다.',
        'SUMMARY_KEYWORDS',
        'pg_bigm + GIN 인덱스로 다중 키워드 검색을 최적화하고, 실행 계획을 통해 seq scan 여부를 점검해야 한다.'
    ),
    (
        '2025-11-19 09:36:10',
        '2025-11-19 09:36:10',
        NULL,  
        8, 
        NULL,
        2, 
        '2025-11-19 09:36:10',
        '장점은 동일 검색어에 대한 응답 시간을 크게 줄이고, DB 부하를 감소시킬 수 있다는 점이다. 단점은 캐시 무효화 전략이 복잡해지고, 메모리 사용량이 늘어나며, 실시간성이 중요한 데이터에는 부적합할 수 있다는 점이다.',
        'CHAT',
        'Redis,캐싱,응답시간,DB부하,캐시무효화',
        '검색 결과를 Redis에 캐싱하면 어떤 장단점이 있을까?',
        '검색 결과를 Redis에 캐싱하면 어떤 장단점이 있을까? '
        '장점은 동일 검색어에 대한 응답 시간을 크게 줄이고, DB 부하를 감소시킬 수 있다는 점이다. 단점은 캐시 무효화 전략이 복잡해지고, 메모리 사용량이 늘어나며, 실시간성이 중요한 데이터에는 부적합할 수 있다는 점이다.',
        'SUMMARY_KEYWORDS',
        'Redis로 검색 결과를 캐싱하면 응답 시간 단축과 DB 부하 감소 효과가 있지만, 캐시 무효화 전략이 복잡해지고 메모리 사용량이 증가하며 실시간성이 중요한 데이터에는 부적합할 수 있다.'
    ),
    (
        '2025-11-19 09:37:10',
        '2025-11-19 09:37:10',
        NULL,   
        11,     
        NULL,
        3,    
        '2025-11-19 09:37:10',
        '더미 데이터로 몇 개의 채팅을 추가해 두면, 검색, 정렬, 페이징, 인덱스 성능 등을 손쉽게 검증할 수 있다.',
        'CHAT',
        '더미데이터,검색,정렬,페이징,테스트',
        '테스트용 채팅 데이터를 몇 개만 넣어보고 싶어.',
        '테스트용 채팅 데이터를 몇 개만 넣어보고 싶어. '
        '더미 데이터로 몇 개의 채팅을 추가해 두면, 검색, 정렬, 페이징, 인덱스 성능 등을 손쉽게 검증할 수 있다.',
        'SUMMARY_KEYWORDS',
        '더미 채팅 데이터를 미리 넣어두면 검색, 정렬, 페이징, 인덱스 기능을 빠르게 테스트할 수 있다.'
    ),
    (
        '2025-11-19 09:38:00',
        '2025-11-19 09:38:00',
        NULL,
        3,
        NULL,
        1,
        '2025-11-19 09:38:00',
        '다중 키워드 검색에서는 WHERE 절의 키워드 순서보다 인덱스 구조와 통계 정보가 더 큰 영향을 준다. 다만 선택도가 높은 키워드를 먼저 사용하는 방식으로 쿼리를 재작성하면 실행 계획이 더 효율적으로 나오는 경우가 있다.',
        'CHAT',
        '다중 키워드,선택도,실행 계획',
        '다중 키워드 검색에서 키워드 순서에 따라 성능 차이가 많이 날까?',
        '다중 키워드 검색에서 키워드 순서에 따라 성능 차이가 많이 날까? 다중 키워드 검색에서는 WHERE 절의 키워드 순서보다 인덱스 구조와 통계 정보가 더 큰 영향을 준다. 다만 선택도가 높은 키워드를 먼저 사용하는 방식으로 쿼리를 재작성하면 실행 계획이 더 효율적으로 나오는 경우가 있다.',
        'SUMMARY_KEYWORDS',
        '다중 키워드 검색의 성능은 키워드 순서보다 인덱스와 통계 정보에 더 영향을 받고, 선택도가 높은 조건을 먼저 적용하도록 쿼리를 재작성하면 도움이 된다.'
    ),
    (
        '2025-11-19 09:38:10',
        '2025-11-19 09:38:10',
        NULL,
        3,
        NULL,
        1,
        '2025-11-19 09:38:10',
        'ILIKE는 대소문자를 구분하지 않는 대신, 일반 LIKE보다 약간 느릴 수 있다. 다만 pg_trgm이나 pg_bigm 같은 확장과 GIN 인덱스를 함께 사용하면 ILIKE 패턴 검색도 충분히 빠르게 처리할 수 있다.',
        'CHAT',
        'LIKE,ILIKE,대소문자,pg_bigm',
        'LIKE 대신 ILIKE를 쓰면 항상 느려지는 걸까?',
        'LIKE 대신 ILIKE를 쓰면 항상 느려지는 걸까? ILIKE는 대소문자를 구분하지 않는 대신, 일반 LIKE보다 약간 느릴 수 있다. 다만 pg_trgm이나 pg_bigm 같은 확장과 GIN 인덱스를 함께 사용하면 ILIKE 패턴 검색도 충분히 빠르게 처리할 수 있다.',
        'SUMMARY_KEYWORDS',
        'ILIKE는 기본 LIKE보다 약간 느릴 수 있지만, 적절한 GIN 인덱스와 확장을 사용하면 충분히 빠른 대소문자 무시 검색이 가능하다.'
    ),
    (
        '2025-11-19 09:38:20',
        '2025-11-19 09:38:20',
        NULL,
        1,
        NULL,
        1,
        '2025-11-19 09:38:20',
        'pg_bigm 인덱스는 쓰기 작업이 많을수록 유지 비용이 커지기 때문에, 대량 삽입이 필요한 경우에는 일시적으로 인덱스를 비활성화하거나 삽입 후에 인덱스를 다시 생성하는 전략도 고려할 수 있다.',
        'CHAT',
        'pg_bigm,인덱스 유지비용,쓰기 부하',
        'pg_bigm 인덱스를 걸어두면 INSERT가 많이 느려질까?',
        'pg_bigm 인덱스를 걸어두면 INSERT가 많이 느려질까? pg_bigm 인덱스는 쓰기 작업이 많을수록 유지 비용이 커지기 때문에, 대량 삽입이 필요한 경우에는 일시적으로 인덱스를 비활성화하거나 삽입 후에 인덱스를 다시 생성하는 전략도 고려할 수 있다.',
        'SUMMARY_KEYWORDS',
        'pg_bigm 인덱스는 읽기 성능을 올려주지만, 쓰기 부하가 큰 환경에서는 인덱스 유지 비용을 고려해 배치 삽입 전략을 세워야 한다.'
    ),
    (
        '2025-11-19 09:38:30',
        '2025-11-19 09:38:30',
        NULL,
        3,
        NULL,
        1,
        '2025-11-19 09:38:30',
        'GIN은 많은 키를 가진 데이터(예: 전체 텍스트 검색)에 강하고, GIST는 범위 검색이나 기하 타입 같은 다양한 자료형에 유연하다. 텍스트 검색 위주라면 GIN이 더 적합한 경우가 많다.',
        'CHAT',
        'GIN,GIST,텍스트 검색,인덱스 선택',
        '텍스트 검색에는 GIN이랑 GIST 중에 보통 뭘 더 많이 써?',
        '텍스트 검색에는 GIN이랑 GIST 중에 보통 뭘 더 많이 써? GIN은 많은 키를 가진 데이터에 강하고, GIST는 범위 검색이나 기하 타입 같은 다양한 자료형에 유연하다. 텍스트 검색 위주라면 GIN이 더 적합한 경우가 많다.',
        'SUMMARY_KEYWORDS',
        '텍스트 기반 검색에는 보통 GIN 인덱스를 우선 고려하고, 특수한 자료형이나 범위 검색이 필요할 때 GIST를 선택하는 편이 좋다.'
    ),
    (
        '2025-11-19 09:38:40',
        '2025-11-19 09:38:40',
        NULL,
        3,
        NULL,
        1,
        '2025-11-19 09:38:40',
        'OFFSET이 큰 페이징 쿼리는 앞부분을 계속 스캔해야 해서 비효율적이다. 커서 기반 페이징이나 마지막 요소의 정렬 키를 기억하는 방식으로 페이징 전략을 바꾸면 성능을 크게 개선할 수 있다.',
        'CHAT',
        'OFFSET,LIMIT,무한스크롤,커서 페이징',
        'OFFSET, LIMIT 방식으로 무한스크롤 하면 많이 느려질까?',
        'OFFSET, LIMIT 방식으로 무한스크롤 하면 많이 느려질까? OFFSET이 큰 페이징 쿼리는 앞부분을 계속 스캔해야 해서 비효율적이다. 커서 기반 페이징이나 마지막 요소의 정렬 키를 기억하는 방식으로 페이징 전략을 바꾸면 성능을 크게 개선할 수 있다.',
        'SUMMARY_KEYWORDS',
        'OFFSET/LIMIT 기반 페이징은 페이지가 뒤로 갈수록 비효율적이므로, 커서 기반 페이징이나 키 기반 페이징으로 전환하는 것이 좋다.'
    ),
    (
        '2025-11-19 09:38:50',
        '2025-11-19 09:38:50',
        NULL,
        8,
        NULL,
        2,
        '2025-11-19 09:38:50',
        'key 테이블에서 member_uid, provider_catalog_uid 조합으로 자주 조회한다면, 이 두 컬럼을 묶은 복합 인덱스를 만드는 것이 좋다. 단일 컬럼 인덱스만 있을 때보다 필터링 효율이 높아진다.',
        'CHAT',
        'key 테이블,복합 인덱스,member_uid',
        'key 테이블에서 특정 멤버와 제공사 조합으로 자주 조회하면 인덱스를 어떻게 잡는 게 좋을까?',
        'key 테이블에서 특정 멤버와 제공사 조합으로 자주 조회하면 인덱스를 어떻게 잡는 게 좋을까? key 테이블에서 member_uid, provider_catalog_uid 조합으로 자주 조회한다면, 이 두 컬럼을 묶은 복합 인덱스를 만드는 것이 좋다. 단일 컬럼 인덱스만 있을 때보다 필터링 효율이 높아진다.',
        'SUMMARY_KEYWORDS',
        'key 테이블 조회가 member_uid와 provider_catalog_uid 조합 위주라면 두 컬럼을 묶은 복합 인덱스를 사용하는 것이 효과적이다.'
    ),
    (
        '2025-11-19 09:39:00',
        '2025-11-19 09:39:00',
        NULL,
        8,
        NULL,
        2,
        '2025-11-19 09:39:00',
        'JSONB는 내부적으로 정규화된 바이너리 포맷으로 저장되기 때문에, 키 검색과 비교 연산에서 JSON보다 훨씬 빠른 성능을 낸다. 대신 쓰기 시점에 파싱 비용이 추가되므로 읽기 중심인지 쓰기 중심인지에 따라 선택해야 한다.',
        'CHAT',
        'JSON,JSONB,읽기성능,쓰기비용',
        'room의 branch_view 같은 필드는 JSON이랑 JSONB 중에 뭘 쓰는 게 좋을까?',
        'room의 branch_view 같은 필드는 JSON이랑 JSONB 중에 뭘 쓰는 게 좋을까? JSONB는 내부적으로 정규화된 바이너리 포맷으로 저장되기 때문에, 키 검색과 비교 연산에서 JSON보다 훨씬 빠른 성능을 낸다. 대신 쓰기 시점에 파싱 비용이 추가되므로 읽기 중심인지 쓰기 중심인지에 따라 선택해야 한다.',
        'SUMMARY_KEYWORDS',
        'branch_view처럼 조회와 필터링 가능성이 있는 JSON 데이터는 대개 JSONB를 사용하는 것이 유리하다.'
    ),
    (
        '2025-11-19 09:39:10',
        '2025-11-19 09:39:10',
        NULL,
        8,
        NULL,
        2,
        '2025-11-19 09:39:10',
        '세션 기반 CSRF 방어에서는 서버가 세션에 토큰을 저장하고, 클라이언트가 헤더나 숨겨진 필드로 같은 값을 보내는지 확인한다. 토큰 발급과 검증이 같은 저장소를 바라보는지만 명확히 맞춰주면 된다.',
        'CHAT',
        'CSRF,세션,토큰,검증',
        '세션 기반 인증에서 CSRF 토큰은 어디에 저장하고 어떻게 검증하는 게 맞을까?',
        '세션 기반 인증에서 CSRF 토큰은 어디에 저장하고 어떻게 검증하는 게 맞을까? 세션 기반 CSRF 방어에서는 서버가 세션에 토큰을 저장하고, 클라이언트가 헤더나 숨겨진 필드로 같은 값을 보내는지 확인한다. 토큰 발급과 검증이 같은 저장소를 바라보는지만 명확히 맞춰주면 된다.',
        'SUMMARY_KEYWORDS',
        '세션 기반 CSRF 토큰은 서버 세션에 저장하고, 클라이언트에서 동일 값을 보내도록 구현해 일관된 저장소 기준으로 검증해야 한다.'
    ),
    (
        '2025-11-19 09:39:20',
        '2025-11-19 09:39:20',
        NULL,
        8,
        NULL,
        2,
        '2025-11-19 09:39:20',
        '세션 타임아웃은 마지막 요청 시각을 기준으로 갱신되기 때문에, 사용자가 활동을 계속하면 세션 만료 시점도 계속 뒤로 밀린다. 다만 브라우저 탭을 완전히 닫고 다시 열면 새로운 세션이 생성될 수 있다.',
        'CHAT',
        '세션,타임아웃,활동기준',
        '세션 기반 인증에서 “사용자의 활동이 있으면 자동 연장된다”는 게 정확히 무슨 의미야?',
        '세션 기반 인증에서 “사용자의 활동이 있으면 자동 연장된다”는 게 정확히 무슨 의미야? 세션 타임아웃은 마지막 요청 시각을 기준으로 갱신되기 때문에, 사용자가 활동을 계속하면 세션 만료 시점도 계속 뒤로 밀린다. 다만 브라우저 탭을 완전히 닫고 다시 열면 새로운 세션이 생성될 수 있다.',
        'SUMMARY_KEYWORDS',
        '세션 타임아웃은 마지막 요청 기준으로 갱신되므로, 사용자가 주기적으로 요청을 보내면 세션이 자동으로 연장된다.'
    ),
    (
        '2025-11-19 09:39:30',
        '2025-11-19 09:39:30',
        NULL,
        8,
        NULL,
        2,
        '2025-11-19 09:39:30',
        'SSE 연결은 HTTP 요청을 장시간 유지하는 방식이라서, 세션 타임아웃 설정에 따라 의도치 않게 세션이 끝나거나 유지될 수 있다. SSE와 일반 요청의 세션 정책을 어떻게 가져갈지 명확하게 설계하는 것이 중요하다.',
        'CHAT',
        'SSE,세션,타임아웃,연결유지',
        'SSE를 쓰면 세션 타임아웃 설정이랑 충돌나는 부분 없을까?',
        'SSE를 쓰면 세션 타임아웃 설정이랑 충돌나는 부분 없을까? SSE 연결은 HTTP 요청을 장시간 유지하는 방식이라서, 세션 타임아웃 설정에 따라 의도치 않게 세션이 끝나거나 유지될 수 있다. SSE와 일반 요청의 세션 정책을 어떻게 가져갈지 명확하게 설계하는 것이 중요하다.',
        'SUMMARY_KEYWORDS',
        'SSE를 사용할 때는 장시간 연결이 세션 타임아웃 정책과 어떻게 상호작용하는지 고려해 별도의 설계가 필요하다.'
    ),
    (
        '2025-11-19 09:39:40',
        '2025-11-19 09:39:40',
        NULL,
        11,
        NULL,
        3,
        '2025-11-19 09:39:40',
        '읽기 비중이 높고 동일한 결과가 자주 재사용되는 시나리오에서는 Redis 캐시가 매우 효과적이다. 다만 캐시 미스율이 높거나 데이터가 자주 변경되면 오히려 복잡도만 올라갈 수 있다.',
        'CHAT',
        'Redis,DB,캐시 전략,읽기 비중',
        '검색 API에 Redis 캐시를 붙이는 게 항상 좋은 건 아닐까?',
        '검색 API에 Redis 캐시를 붙이는 게 항상 좋은 건 아닐까? 읽기 비중이 높고 동일한 결과가 자주 재사용되는 시나리오에서는 Redis 캐시가 매우 효과적이다. 다만 캐시 미스율이 높거나 데이터가 자주 변경되면 오히려 복잡도만 올라갈 수 있다.',
        'SUMMARY_KEYWORDS',
        'Redis 캐시는 읽기 빈도와 재사용도가 높은 경우에 적합하며, 변경이 잦거나 패턴이 다양하면 이득이 줄어든다.'
    ),
    (
        '2025-11-19 09:39:50',
        '2025-11-19 09:39:50',
        NULL,
        11,
        NULL,
        3,
        '2025-11-19 09:39:50',
        'CQRS 패턴에서 읽기 DB는 조회 전용으로 튜닝할 수 있어서 인덱스를 공격적으로 추가하거나 캐시 계층과 통합하기 좋다. 대신 쓰기 지연과 데이터 일관성 수준을 어디까지 허용할지 결정해야 한다.',
        'CHAT',
        'CQRS,읽기 DB,일관성,튜닝',
        '채팅 검색용으로 읽기 전용 DB를 따로 두면 어떤 장단점이 있을까?',
        '채팅 검색용으로 읽기 전용 DB를 따로 두면 어떤 장단점이 있을까? CQRS 패턴에서 읽기 DB는 조회 전용으로 튜닝할 수 있어서 인덱스를 공격적으로 추가하거나 캐시 계층과 통합하기 좋다. 대신 쓰기 지연과 데이터 일관성 수준을 어디까지 허용할지 결정해야 한다.',
        'SUMMARY_KEYWORDS',
        'CQRS로 읽기 DB를 분리하면 조회 성능을 극대화할 수 있지만, 쓰기 지연과 일관성 이슈를 설계 단계에서 고려해야 한다.'
    ),
    (
        '2025-11-19 09:40:00',
        '2025-11-19 09:40:00',
        NULL,
        11,
        NULL,
        3,
        '2025-11-19 09:40:00',
        'EXPLAIN만 보면 예상 실행 계획만 확인할 수 있고, EXPLAIN ANALYZE를 써야 실제 수행 시간과 각 노드의 비용을 볼 수 있다. 다만 ANALYZE는 실제로 쿼리를 실행하므로, 대량 변경 쿼리에는 주의가 필요하다.',
        'CHAT',
        'EXPLAIN,ANALYZE,실행 계획,성능 분석',
        '실행 계획 볼 때 EXPLAIN이랑 EXPLAIN ANALYZE는 언제 구분해서 써야 해?',
        '실행 계획 볼 때 EXPLAIN이랑 EXPLAIN ANALYZE는 언제 구분해서 써야 해? EXPLAIN만 보면 예상 실행 계획만 확인할 수 있고, EXPLAIN ANALYZE를 써야 실제 수행 시간과 각 노드의 비용을 볼 수 있다. 다만 ANALYZE는 실제로 쿼리를 실행하므로, 대량 변경 쿼리에는 주의가 필요하다.',
        'SUMMARY_KEYWORDS',
        'EXPLAIN은 계획만, EXPLAIN ANALYZE는 실제 실행 결과까지 보여주므로, 읽기 쿼리 튜닝에는 ANALYZE가 특히 유용하다.'
    ),
    (
        '2025-11-19 09:40:10',
        '2025-11-19 09:40:10',
        NULL,
        11,
        NULL,
        3,
        '2025-11-19 09:40:10',
        'autovacuum는 지워진 튜플을 정리하고 통계를 갱신해서 쿼리 성능을 유지하는 역할을 한다. 너무 공격적으로 끄거나 지연시키면, 오래된 통계와 bloating 때문에 성능이 급격히 나빠질 수 있다.',
        'CHAT',
        'autovacuum,통계,성능저하,bloat',
        'PostgreSQL에서 autovacuum을 끄면 어떤 일이 벌어질까?',
        'PostgreSQL에서 autovacuum을 끄면 어떤 일이 벌어질까? autovacuum는 지워진 튜플을 정리하고 통계를 갱신해서 쿼리 성능을 유지하는 역할을 한다. 너무 공격적으로 끄거나 지연시키면, 오래된 통계와 bloating 때문에 성능이 급격히 나빠질 수 있다.',
        'SUMMARY_KEYWORDS',
        'autovacuum는 성능 유지를 위해 필수적인 기능이라, 무작정 끄기보다는 튜닝해서 사용하는 것이 안전하다.'
    ),
    (
        '2025-11-19 09:40:20',
        '2025-11-19 09:40:20',
        NULL,
        11,
        NULL,
        3,
        '2025-11-19 09:40:20',
        '업데이트와 삭제가 많이 발생하면 dead tuple이 쌓이는데, 이것들이 제대로 정리되지 않으면 테이블 크기가 불필요하게 커지고 인덱스 스캔 비용도 증가한다. 주기적인 VACUUM과 autovacuum 설정이 중요하다.',
        'CHAT',
        'dead tuple,VACUUM,테이블 크기',
        'dead tuple이 많이 쌓이면 실제로 어떤 식으로 성능에 악영향을 줘?',
        'dead tuple이 많이 쌓이면 실제로 어떤 식으로 성능에 악영향을 줘? 업데이트와 삭제가 많이 발생하면 dead tuple이 쌓이는데, 이것들이 제대로 정리되지 않으면 테이블 크기가 불필요하게 커지고 인덱스 스캔 비용도 증가한다. 주기적인 VACUUM과 autovacuum 설정이 중요하다.',
        'SUMMARY_KEYWORDS',
        'dead tuple 누적은 테이블과 인덱스의 실질 크기를 키워 쿼리 성능을 떨어뜨리므로, VACUUM 전략이 중요하다.'
    ),
    (
        '2025-11-19 09:40:30',
        '2025-11-19 09:40:30',
        NULL,
        11,
        NULL,
        3,
        '2025-11-19 09:40:30',
        '커넥션 풀의 크기를 너무 크게 잡으면 DB가 동시에 처리할 수 있는 요청 수를 넘어설 때 컨텍스트 스위칭 비용만 늘어난다. 보통 CPU 코어 수와 워크로드 패턴을 기준으로 적정 값을 찾는 게 좋다.',
        'CHAT',
        '커넥션 풀,DB 커넥션,성능',
        'Spring에서 HikariCP 커넥션 풀 사이즈는 어떤 기준으로 정하는 게 좋을까?',
        'Spring에서 HikariCP 커넥션 풀 사이즈는 어떤 기준으로 정하는 게 좋을까? 커넥션 풀의 크기를 너무 크게 잡으면 DB가 동시에 처리할 수 있는 요청 수를 넘어설 때 컨텍스트 스위칭 비용만 늘어난다. 보통 CPU 코어 수와 워크로드 패턴을 기준으로 적정 값을 찾는 게 좋다.',
        'SUMMARY_KEYWORDS',
        '커넥션 풀 크기는 “많을수록 좋다”가 아니라 DB와 서버 리소스를 고려해 적정 수준으로 설정해야 한다.'
    ),
    (
        '2025-11-19 09:40:40',
        '2025-11-19 09:40:40',
        NULL,
        11,
        NULL,
        3,
        '2025-11-19 09:40:40',
        '슬로우 쿼리 로그를 적절한 임계값으로 활성화해두면, 실제 트래픽에서 어떤 쿼리가 병목이 되는지 빠르게 파악할 수 있다. 다만 임계값을 너무 낮게 두면 로그가 과도하게 쌓여서 분석이 어려워질 수 있다.',
        'CHAT',
        '슬로우 쿼리 로그,병목,성능 모니터링',
        '실서비스에서 슬로우 쿼리 로그는 어느 정도 기준으로 켜두는 게 좋을까?',
        '실서비스에서 슬로우 쿼리 로그는 어느 정도 기준으로 켜두는 게 좋을까? 슬로우 쿼리 로그를 적절한 임계값으로 활성화해두면, 실제 트래픽에서 어떤 쿼리가 병목이 되는지 빠르게 파악할 수 있다. 다만 임계값을 너무 낮게 두면 로그가 과도하게 쌓여서 분석이 어려워질 수 있다.',
        'SUMMARY_KEYWORDS',
        '슬로우 쿼리 로그는 병목 쿼리를 찾는 강력한 도구지만, 임계값과 로그 관리 전략을 함께 고려해야 한다.'
    ),
    (
        '2025-11-19 09:40:50',
        '2025-11-19 09:40:50',
        NULL,
        11,
        NULL,
        3,
        '2025-11-19 09:40:50',
        '스키마 마이그레이션 도구를 도입하면 환경별 스키마 차이를 줄이고 배포 과정을 자동화할 수 있다. 다만 초기에는 기존 DB 상태를 정리하고 baseline을 맞추는 작업이 필요하다.',
        'CHAT',
        '마이그레이션,스키마,배포 자동화',
        'schema.sql에서 Flyway 같은 마이그레이션 도구로 옮길 때 어떤 점을 신경 써야 할까?',
        'schema.sql에서 Flyway 같은 마이그레이션 도구로 옮길 때 어떤 점을 신경 써야 할까? 스키마 마이그레이션 도구를 도입하면 환경별 스키마 차이를 줄이고 배포 과정을 자동화할 수 있다. 다만 초기에는 기존 DB 상태를 정리하고 baseline을 맞추는 작업이 필요하다.',
        'SUMMARY_KEYWORDS',
        '마이그레이션 도구 도입 시에는 기존 스키마 상태를 정리하고 baseline 버전을 명확히 정의하는 것이 중요하다.'
    ),
    (
        '2025-11-19 09:41:00',
        '2025-11-19 09:41:00',
        NULL,
        11,
        NULL,
        3,
        '2025-11-19 09:41:00',
        '더미 데이터도 실제 서비스에서 나올 법한 길이와 키워드를 포함하면, 인덱스 선택이나 캐시 히트율 같은 지표를 더 현실적으로 검증할 수 있다. 단순한 반복 문자열은 실제 패턴과 동떨어진 결과를 만들 수 있다.',
        'CHAT',
        '더미 데이터,현실성,테스트',
        '검색 성능 테스트용 더미 데이터를 만들 때도 현실성을 어느 정도 신경 써야 할까?',
        '검색 성능 테스트용 더미 데이터를 만들 때도 현실성을 어느 정도 신경 써야 할까? 더미 데이터도 실제 서비스에서 나올 법한 길이와 키워드를 포함하면, 인덱스 선택이나 캐시 히트율 같은 지표를 더 현실적으로 검증할 수 있다. 단순한 반복 문자열은 실제 패턴과 동떨어진 결과를 만들 수 있다.',
        'SUMMARY_KEYWORDS',
        '현실적인 더미 데이터는 검색·인덱스·캐시 전략을 검증할 때 실제 서비스에 가까운 결과를 얻게 해준다.'
    );

-- =========================================================
-- chat 복제본 생성 (origin → 복제본)
--  - origin 기준: group_id IS NULL
--  - 복제본: group_id != NULL, origin_id = origin.chat_uid
-- =========================================================

INSERT INTO public.chat
    (answered_at,
     created_at,
     group_id,
     model_catalog_uid,
     origin_id,
     room_id,
     updated_at,
     answer,
     is_chat,
     keywords,
     question,
     search_content,
     status,
     summary)
SELECT
    c.answered_at,
    c.created_at,
    CASE
        WHEN c.room_id = 1 THEN 1
        WHEN c.room_id = 2 THEN 2 
        ELSE 1                    
    END AS group_id,
    c.model_catalog_uid,
    c.chat_uid       AS origin_id,
    c.room_id,
    c.updated_at,
    c.answer,
    c.is_chat,
    c.keywords,
    c.question,
    c.search_content,
    c.status,
    c.summary
FROM public.chat c
WHERE c.group_id IS NULL;
