import re
import torch
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
from keybert import KeyBERT
from krwordrank.word import KRWordRank

# ----------------------------------------------------
# âœ… ê³µê°œë¡œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ ID
#   - ìš”ì•½: gogamza/kobart-summarization
#   - ì œëª©(ì§§ì€ ë‰´ìŠ¤ íƒ€ì´í‹€): gogamza/kobart-title
# ----------------------------------------------------
SUM_MODEL_NAME = "gogamza/kobart-summarization"
TITLE_MODEL_NAME = "heegyu/kobart-title"

print("ëª¨ë¸ ë¡œë”© ì¤‘...")

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def _load_model(model_name: str):
    tok = PreTrainedTokenizerFast.from_pretrained(model_name)
    mdl = BartForConditionalGeneration.from_pretrained(model_name).to(DEVICE)
    return tok, mdl

try:
    sum_tokenizer, sum_model = _load_model(SUM_MODEL_NAME)
except Exception as e:
    print(f"[WARN] ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨({SUM_MODEL_NAME}): {e}. digit82/kobart-summarization ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
    SUM_MODEL_NAME = "digit82/kobart-summarization"
    sum_tokenizer, sum_model = _load_model(SUM_MODEL_NAME)

try:
    title_tokenizer, title_model = _load_model(TITLE_MODEL_NAME)
except Exception as e:
    print(f"[WARN] ì œëª© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨({TITLE_MODEL_NAME}): {e}. ìš”ì•½ ëª¨ë¸ ì¬ì‚¬ìš©ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
    title_tokenizer, title_model = sum_tokenizer, sum_model

kw_model = KeyBERT(model="jhgan/ko-sroberta-multitask")

print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device={DEVICE})")
print(f"- SUMMARY: {SUM_MODEL_NAME}")
print(f"- TITLE  : {TITLE_MODEL_NAME if title_model is not sum_model else SUM_MODEL_NAME}")

# ----------------------------------------------------
# ğŸ”§ ì „ì²˜ë¦¬
# ----------------------------------------------------
def preprocess_text(text: str) -> str:
    """ë¶ˆí•„ìš” ê³µë°±/ë”°ì˜´í‘œ ì œê±° + ë¬¸ì¥ ë‹¨ìœ„ë¡œ 3~4ê°œë§Œ ì‚¬ìš©"""
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"[\"\'â€˜â€™â€œâ€]", "", text)
    sentences = re.split(r"(?<=[.!?])\s+", text)
    return " ".join(sentences[:4])

# ----------------------------------------------------
# ğŸ“ ìš”ì•½
# ----------------------------------------------------
def generate_summary(text: str, max_len: int = 150, min_len: int = 50) -> str:
    text = preprocess_text(text)
    input_ids = sum_tokenizer.encode(
        text, return_tensors="pt", max_length=1024, truncation=True
    ).to(DEVICE)

    summary_ids = sum_model.generate(
        input_ids,
        max_length=max_len,
        min_length=min_len,
        num_beams=4,
        repetition_penalty=3.0,
        length_penalty=1.3,
        no_repeat_ngram_size=5,
        early_stopping=True,
    )

    summary = sum_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    summary = re.sub(r"\s+", " ", summary).strip()

    # ì¤‘ë³µ ë¬¸ì¥ ì œê±°
    parts = [p.strip() for p in re.split(r"[.]\s*", summary) if p.strip()]
    seen = set()
    dedup = []
    for p in parts:
        if p not in seen:
            seen.add(p)
            dedup.append(p)
    return ". ".join(dedup).strip()

# ----------------------------------------------------
# ğŸ” í‚¤ì›Œë“œ (KRWordRank + KeyBERT í•˜ì´ë¸Œë¦¬ë“œ)
# ----------------------------------------------------
def clean_keyword(kw: str) -> str:
    kw = re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", "", kw)
    kw = re.sub(r"\s+", " ", kw)
    kw = re.sub(r"(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ìœ¼ë¡œ|ì™€|ê³¼|ì˜|ë„|ë¡œ|í•˜ë‹¤|ëœë‹¤|í–ˆë‹¤)$", "", kw)
    return kw.strip()

def extract_keywords(text: str) -> list[str]:
    try:
        # KRWordRank
        wordrank_extractor = KRWordRank(min_count=2, max_length=10, verbose=False)
        sents = [s.strip() for s in re.split(r"[.!?]\s*", text) if s.strip()]
        if not sents:
            sents = [text]
        kw_scores, _, _ = wordrank_extractor.extract(sents, beta=0.85, max_iter=10)
        kr_kws = list(kw_scores.keys())[:5]

        # KeyBERT
        kb_pairs = kw_model.extract_keywords(text, top_n=7, keyphrase_ngram_range=(1, 3), use_mmr=True, diversity=0.9)
        keybert_kws = [p[0] for p in kb_pairs]

        # ë³‘í•© + ì •ì œ
        merged = []
        for kw in kr_kws + keybert_kws:
            c = clean_keyword(kw)
            if 2 <= len(c) <= 20 and c not in merged:
                merged.append(c)
            if len(merged) >= 5:
                break
        return merged
    except Exception as e:
        print(f"[Error] Keyword extraction failed: {e}")
        return []

# ----------------------------------------------------
# ğŸ“° ì œëª© (ì§§ì€ ë‰´ìŠ¤ íƒ€ì´í‹€í˜•)
# ----------------------------------------------------
def generate_title(text: str, max_len: int = 20, min_len: int = 5) -> str:
    """ì§§ê³  ìì—°ìŠ¤ëŸ¬ìš´ headline ìƒì„± (ì„œìˆ í˜• â†’ ëª…ì‚¬êµ¬ ë³€í™˜ í›„ì²˜ë¦¬ í¬í•¨)"""
    input_text = preprocess_text(text)
    input_ids = title_tokenizer.encode(
        input_text, return_tensors="pt", max_length=512, truncation=True
    ).to(DEVICE)

    out_ids = title_model.generate(
        input_ids,
        max_length=max_len,
        min_length=min_len,
        num_beams=4,
        repetition_penalty=2.5,
        length_penalty=1.1,
        no_repeat_ngram_size=4,
        early_stopping=True,
    )

    # 1ï¸âƒ£ ëª¨ë¸ ê²°ê³¼ ë””ì½”ë”©
    title = title_tokenizer.decode(out_ids[0], skip_special_tokens=True)
    title = re.sub(r"\s+", " ", title).strip()

    # 2ï¸âƒ£ ì„œìˆ í˜• â†’ headlineí˜•ìœ¼ë¡œ ë³€í™˜
    # âœ… ì¡°ì‚¬ ì œê±°: ë‹¨ì–´ì™€ ë¶„ë¦¬ëœ ì¡°ì‚¬ë§Œ ì œê±°
    title = re.sub(r"(ìœ¼ë¡œ|ë¡œ|ì—ì„œ|ì—|ì—ê²Œ|ì„|ë¥¼|ì€|ëŠ”|ì´|ê°€|ì™€|ê³¼)(\b|[ê°€-í£])", " ", title)
    title = re.sub(r"(ê³¼ í•¨ê»˜|ì™€ í•¨ê»˜|í•¨ê»˜)(\s|$)", "", title)
    title = re.sub(r"(ë‹¤|ëœë‹¤|ë°í˜”ë‹¤|ì˜ˆì •ì´ë‹¤|ê²ƒìœ¼ë¡œ|ë¶„ì„ëœë‹¤|ë‚˜íƒ€ë‚¬ë‹¤|í–ˆë‹¤)$", "", title)
    title = re.sub(r"(ê²ƒì´|ì£¼ëœ ìš”ì¸|ì£¼ìš” ì›ì¸|ìœ¼ë¡œ ë¶„ì„)$", "", title)
    title = re.sub(r"\s+", " ", title).strip(" .,")

    # 3ï¸âƒ£ ê¸¸ì´ ì •ë¦¬
    words = title.split()
    if len(words) > 4:
        title = " ".join(words[:4])
    if len(title) > 25:
        title = title[:25].rsplit(" ", 1)[0]

    return title
