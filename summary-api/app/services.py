import re
import torch
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
from keybert import KeyBERT
from krwordrank.word import KRWordRank

# ----------------------------------------------------
# âœ… ê³µê°œë¡œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ ID
#   - ìš”ì•½: gogamza/kobart-summarization
#   - ì œëª©(ì§§ì€ ë‰´ìŠ¤ íƒ€ì´í‹€): gogamza/kobart-title
# ----------------------------------------------------
SUM_MODEL_NAME = "gogamza/kobart-summarization"
TITLE_MODEL_NAME = "heegyu/kobart-title"

print("ëª¨ë¸ ë¡œë”© ì¤‘...")

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def _load_model(model_name: str):
    tok = PreTrainedTokenizerFast.from_pretrained(model_name)
    mdl = BartForConditionalGeneration.from_pretrained(model_name).to(DEVICE)
    return tok, mdl

try:
    sum_tokenizer, sum_model = _load_model(SUM_MODEL_NAME)
except Exception as e:
    print(f"[WARN] ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨({SUM_MODEL_NAME}): {e}. digit82/kobart-summarization ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
    SUM_MODEL_NAME = "digit82/kobart-summarization"
    sum_tokenizer, sum_model = _load_model(SUM_MODEL_NAME)

try:
    title_tokenizer, title_model = _load_model(TITLE_MODEL_NAME)
except Exception as e:
    print(f"[WARN] ì œëª© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨({TITLE_MODEL_NAME}): {e}. ìš”ì•½ ëª¨ë¸ ì¬ì‚¬ìš©ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
    title_tokenizer, title_model = sum_tokenizer, sum_model

kw_model = KeyBERT(model="jhgan/ko-sroberta-multitask")

print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device={DEVICE})")
print(f"- SUMMARY: {SUM_MODEL_NAME}")
print(f"- TITLE  : {TITLE_MODEL_NAME if title_model is not sum_model else SUM_MODEL_NAME}")

# ----------------------------------------------------
# ğŸ”§ ì „ì²˜ë¦¬
# ----------------------------------------------------
def preprocess_text(text: str) -> str:
    """ë¶ˆí•„ìš” ê³µë°±/ë”°ì˜´í‘œ ì œê±° + ë¬¸ì¥ ë‹¨ìœ„ë¡œ 3~4ê°œë§Œ ì‚¬ìš©"""
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"[\"\'â€˜â€™â€œâ€]", "", text)
    sentences = re.split(r"(?<=[.!?])\s+", text)
    return " ".join(sentences[:4])

# ----------------------------------------------------
# ğŸ“ ìš”ì•½
# ----------------------------------------------------
def _get_sentence_keywords(sentence: str) -> set:
    """ë¬¸ì¥ì—ì„œ í•µì‹¬ ëª…ì‚¬ ì¶”ì¶œ (ìœ ì‚¬ ë¬¸ì¥ ê°ì§€ìš©)"""
    words = re.findall(r'[ê°€-í£]{2,}', sentence)
    # ì¡°ì‚¬/ì–´ë¯¸ ì œê±°í•˜ê³  ëª…ì‚¬ë§Œ ì¶”ì¶œ
    keywords = set()
    for w in words:
        w = re.sub(r'(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì„œ|ìœ¼ë¡œ|ì™€|ê³¼|ì˜|ë„|ë¡œ|ê³ |ë©°|ë‹¤)$', '', w)
        if len(w) >= 2:
            keywords.add(w)
    return keywords

def generate_summary(text: str, max_len: int = 150, min_len: int = 50) -> str:
    """í…ìŠ¤íŠ¸ ìš”ì•½ + ì¤‘ë³µ/ìœ ì‚¬ ë¬¸ì¥ ì œê±° (ê°œì„  ë²„ì „)"""
    text = preprocess_text(text)
    input_ids = sum_tokenizer.encode(
        text, return_tensors="pt", max_length=1024, truncation=True
    ).to(DEVICE)

    summary_ids = sum_model.generate(
        input_ids,
        max_length=max_len,
        min_length=min_len,
        num_beams=5,  # beam ìˆ˜ ì¦ê°€ (ë” ë§ì€ í›„ë³´ íƒìƒ‰)
        repetition_penalty=4.0,  # ë°˜ë³µ í˜ë„í‹° ë” ì¦ê°€ (ì›ë¬¸ ë³µì‚¬ ë°©ì§€)
        length_penalty=1.0,  # ê¸¸ì´ í˜ë„í‹° ì¤„ì„ (ë„ˆë¬´ ê¸¸ì–´ì§€ëŠ” ê±¸ ë°©ì§€)
        no_repeat_ngram_size=3,  # 3-gram ë°˜ë³µ ë°©ì§€
        early_stopping=True,
    )

    summary = sum_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    summary = re.sub(r"\s+", " ", summary).strip()

    # ì¤‘ë³µ/ìœ ì‚¬ ë¬¸ì¥ ì œê±° (ê°œì„ )
    parts = [p.strip() for p in re.split(r"[.!?]\s*", summary) if p.strip()]
    dedup = []
    seen_exact = set()
    seen_keywords = []

    for p in parts:
        # 1. ì™„ì „ ë™ì¼í•œ ë¬¸ì¥ ì œê±°
        if p in seen_exact:
            continue

        # 2. ìœ ì‚¬ ë¬¸ì¥ ì œê±° (70% ì´ìƒ í‚¤ì›Œë“œ ê²¹ì¹˜ë©´ ì œì™¸)
        p_keywords = _get_sentence_keywords(p)
        is_similar = False
        for prev_keywords in seen_keywords:
            if not p_keywords or not prev_keywords:
                continue
            overlap = len(p_keywords & prev_keywords)
            similarity = overlap / min(len(p_keywords), len(prev_keywords))
            if similarity > 0.7:  # 70% ì´ìƒ ìœ ì‚¬
                is_similar = True
                break

        if not is_similar:
            dedup.append(p)
            seen_exact.add(p)
            seen_keywords.append(p_keywords)

    result = ". ".join(dedup).strip()
    return result if result else summary

# ----------------------------------------------------
# ğŸ” í‚¤ì›Œë“œ (KRWordRank + KeyBERT í•˜ì´ë¸Œë¦¬ë“œ)
# ----------------------------------------------------

# í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (í‚¤ì›Œë“œì—ì„œ ì œì™¸í•  ë‹¨ì–´ë“¤)
STOPWORDS = {
    # ì§€ì‹œì–´
    "ì´ë²ˆ", "ì €ë²ˆ", "ë‹¤ìŒ", "ì´", "ê·¸", "ì €", "ì´ëŸ°", "ê·¸ëŸ°", "ì €ëŸ°", "ì´ëŸ¬í•œ", "ê·¸ëŸ¬í•œ",
    "ì—¬ê¸°", "ê±°ê¸°", "ì €ê¸°", "ì´ê²ƒ", "ê·¸ê²ƒ", "ì €ê²ƒ", "ì´ê³³", "ê·¸ê³³", "ì €ê³³", "ìƒˆë¡œìš´",
    # ì¼ë°˜ ë™ì‚¬/í˜•ìš©ì‚¬
    "í•˜ë‹¤", "ë˜ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "ì´ë‹¤", "ì•„ë‹ˆë‹¤", "ê°™ë‹¤", "ë³´ë‹¤", "ë§í•˜ë‹¤",
    "ë°œí‘œ", "ì „ë§", "ì˜ˆìƒ", "ê³„íš", "ì¤€ë¹„", "ì§„í–‰", "ì‹¤ì‹œ", "ì¶”ì§„", "ê²€í† ",
    "ê³µê°œ", "ì‚¬ìš©", "íƒ‘ì¬", "í–¥ìƒ", "ê°œë°œ", "ì¶œì‹œ", "ì ìš©", "ì±„íƒ",
    # ì‹œê°„/ìœ„ì¹˜ ë¶€ì‚¬
    "ìµœê·¼", "ì˜¤ëŠ˜", "ì–´ì œ", "ë‚´ì¼", "ì˜¬í•´", "ì‘ë…„", "ë‚´ë…„", "í•­ìƒ", "ê°€ë”", "ìì£¼",
    "ìƒë°˜ê¸°", "í•˜ë°˜ê¸°", "ë¶„ê¸°", "ì—°ë§", "ì—°ì´ˆ", "í˜„ì¬", "ê³¼ê±°", "ë¯¸ë˜",
    # ì •ë„ ë¶€ì‚¬ (ì¶”ê°€)
    "ëŒ€í­", "í¬ê²Œ", "ë§ì´", "ì ê²Œ", "ë§¤ìš°", "ì•„ì£¼", "ì •ë§", "ë„ˆë¬´", "ìƒë‹¹íˆ", "êµ‰ì¥íˆ",
    # ì ‘ì†ì‚¬/ì¡°ì‚¬
    "ê·¸ë¦¬ê³ ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ë˜í•œ", "ë˜", "ë°", "ë“±", "ê²½ìš°", "ë•Œë¬¸",
    # ê¸°íƒ€ ì¼ë°˜ì–´
    "ê²ƒ", "ìˆ˜", "ë“±", "ì ", "ë•Œ", "ê³³", "ì¤‘", "ë‚´", "ì™¸", "ê°„", "ìƒ", "ëŒ€", "ì „", "í›„",
    "ìœ„", "ì•„ë˜", "ì•ˆ", "ë°–", "ì•", "ë’¤", "ì†Œì¬", "ì„¼ì„œ"
}

def clean_keyword(kw: str) -> str:
    """í‚¤ì›Œë“œ ì •ì œ: ëª…ì‚¬ì˜ ì¡°ì‚¬ë§Œ ì œê±° (ë™ì‚¬/í˜•ìš©ì‚¬ ì–´ë¯¸ëŠ” ë³´ì¡´)"""
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ìˆ«ìëŠ” ìœ ì§€)
    kw = re.sub(r"[^ê°€-í£a-zA-Z0-9\s%]", "", kw)
    kw = re.sub(r"\s+", " ", kw)

    # ì¡°ì‚¬ ì œê±° ì „ëµ:
    # - ë™ì‚¬/í˜•ìš©ì‚¬ë¡œ ëë‚˜ëŠ” ê²½ìš° ì¡°ì‚¬ ì œê±° ì•ˆ í•¨
    # - ì˜ˆ: "ê°ì¶•í•˜ëŠ”", "ë˜ëŠ”", "ìˆëŠ”" ë“±ì€ ê·¸ëŒ€ë¡œ ìœ ì§€

    # "ëŠ”"/"ì€" ì²˜ë¦¬: ë™ì‚¬í˜• (ìš©ì–¸+ëŠ”)ì´ ì•„ë‹ ë•Œë§Œ ì œê±°
    # ìš©ì–¸ìœ¼ë¡œ ëë‚˜ëŠ” íŒ¨í„´: ~í•˜ëŠ”, ~ë˜ëŠ”, ~ìˆëŠ”, ~ìŠ¤ëŠ”, ~ë¥´ëŠ” ë“±
    if not re.search(r'[ê°€-í£]{2,}[í•˜ë˜ìˆì—†ê°€ì˜¤ë³´ìŠ¤ë¥´]ëŠ”$', kw):
        kw = re.sub(r"(ì€|ëŠ”)$", "", kw)

    # ë‚˜ë¨¸ì§€ ì¡°ì‚¬ ì œê±° (ëª…ì‚¬ ë’¤ì—ë§Œ)
    kw = re.sub(r"(ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ì—ê²Œ|ìœ¼ë¡œ|ë¡œ|ì™€|ê³¼|ì˜|ë„|ë§Œ|ë¶€í„°|ê¹Œì§€)$", "", kw)

    return kw.strip()

def is_valid_keyword(kw: str) -> bool:
    """í‚¤ì›Œë“œ ìœ íš¨ì„± ê²€ì‚¬: ë¶ˆìš©ì–´, ìˆ«ìë§Œ, ë¶ˆì™„ì „í•œ í‘œí˜„ í•„í„°ë§ (ì—„ê²© ë²„ì „)"""
    if not kw or len(kw) < 2:
        return False

    # ë¶ˆìš©ì–´ ì²´í¬
    if kw.lower() in STOPWORDS:
        return False

    # ê° ë‹¨ì–´ë³„ë¡œ ë¶ˆìš©ì–´ ì²´í¬ (ë³µí•©ëª…ì‚¬ ë‚´ ë¶ˆìš©ì–´ ì œê±°)
    words = kw.split()
    for word in words:
        if word.lower() in STOPWORDS:
            return False

    # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸ (ì˜ˆ: "15", "30")
    if re.match(r"^\d+$", kw):
        return False

    # ìˆ«ì+ê¸°í˜¸ë§Œ ìˆëŠ” ê²½ìš° ì œì™¸ (ì˜ˆ: "15%", "30%", "3.5")
    if re.match(r"^[\d\.\,\%]+$", kw):
        return False

    # ìˆœìˆ˜ ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì¼ í‚¤ì›Œë“œ ì œì™¸ (ì˜ˆ: "30", "2023")
    # ë‹¨, "3ë‚˜ë…¸", "5G" ê°™ì€ ê±´ í—ˆìš©
    if re.match(r"^\d+$", kw.split()[0]) if len(kw.split()) == 1 else False:
        return False

    # ì¡°ì‚¬ë¡œ ëë‚˜ëŠ” ê²½ìš° ì œì™¸ (í™•ì¥)
    if re.search(r"(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—ì„œ|ì—ê²Œ|ìœ¼ë¡œ|ë¡œ|ì™€|ê³¼|ë„|ë§Œ|ê³ |ë©°|ì„œ|ë‹ˆë‹¤|ìŠµë‹ˆë‹¤|í–ˆë‹¤|ëë‹¤)$", kw):
        return False

    # ë™ì‚¬/ë¶€ì‚¬ í˜•íƒœë¡œ ëë‚˜ëŠ” ê²½ìš° ì œì™¸
    if re.search(r"(ë¹ ë¥´ê²Œ|í¬ê²Œ|ë§ì´|ì ê²Œ|ë§¤ìš°|ì•„ì£¼|ì •ë§|ë„ˆë¬´|ì¡°ê¸ˆ|ëŒ€ë¹„|ì‹œì¼°ë‹¤|ì‹œì¼°ìœ¼ë©°)$", kw):
        return False

    # ë™ì‚¬ ì–´ê°„ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš° ì œì™¸
    if re.search(r"(í–¥ìƒ|ì†Œë¹„|ê°œë°œ|ê¸°ìˆ )$", kw) and kw.count(" ") > 0:
        # ë‹¨ì¼ ë‹¨ì–´ë©´ OK, ë³µí•©ì–´ë©´ ì˜ì‹¬ìŠ¤ëŸ¬ì›€ (ì˜ˆ: "ì„±ëŠ¥ì€ 15 í–¥ìƒ")
        return False

    # ê¸¸ì´ ì œí•œ (ìµœëŒ€ 15ìë¡œ ì¤„ì„)
    if len(kw) > 15:
        return False

    # í•œê¸€ì´ í•˜ë‚˜ë„ ì—†ê³  ì˜ë¬¸/ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸ (ì˜ë¬¸ ì•½ì–´ 2ì ì´ìƒì€ í—ˆìš©)
    if not re.search(r"[ê°€-í£]", kw):
        # ì˜ë¬¸ë§Œ ìˆê³  2ì ë¯¸ë§Œì´ë©´ ì œì™¸
        if re.match(r"^[a-zA-Z]+$", kw) and len(kw) < 2:
            return False
        # ìˆ«ìê°€ í¬í•¨ëœ ê²½ìš° í•œê¸€ í•„ìˆ˜
        if re.search(r"\d", kw):
            return False

    return True

def extract_keywords(text: str) -> list[str]:
    """ë²”ìš©ì  í‚¤ì›Œë“œ ì¶”ì¶œ: ëª…ì‚¬ ì¤‘ì‹¬, ê· í˜•ì¡íŒ í•„í„°ë§"""
    try:
        # 1ï¸âƒ£ KRWordRank (í•œêµ­ì–´ ëª…ì‚¬ ì¶”ì¶œ)
        wordrank_extractor = KRWordRank(min_count=1, max_length=10, verbose=False)
        sents = [s.strip() for s in re.split(r"[.!?]\s*", text) if s.strip()]
        if not sents:
            sents = [text]
        kw_scores, _, _ = wordrank_extractor.extract(sents, beta=0.85, max_iter=10)

        # ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        kr_kws = sorted(kw_scores.items(), key=lambda x: x[1], reverse=True)
        kr_kws = [kw for kw, score in kr_kws[:30]]

        # 2ï¸âƒ£ KeyBERT (ì˜ë¯¸ ê¸°ë°˜, ì ìˆ˜ í¬í•¨)
        kb_pairs = kw_model.extract_keywords(
            text,
            top_n=20,
            keyphrase_ngram_range=(1, 1),  # ë‹¨ì¼ì–´ë§Œ
            use_mmr=True,
            diversity=0.7
        )
        # KeyBERTëŠ” ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ì¤‘ìš”
        keybert_kws_with_score = [(kw, score) for kw, score in kb_pairs]

        # 3ï¸âƒ£ ì •ì œ ë° ì„ ë³„
        merged = []
        seen = set()

        # KeyBERT ìš°ì„  (ì˜ë¯¸ ê¸°ë°˜ì´ë¼ í’ˆì§ˆ ë†’ìŒ)
        for kw, score in keybert_kws_with_score:
            if len(merged) >= 5:
                break

            cleaned = clean_keyword(kw)
            if not cleaned or len(cleaned) < 2:
                continue

            if cleaned in seen:
                continue

            # ê³µë°± ê¸ˆì§€
            if " " in cleaned:
                continue

            # ì¡°ì‚¬ ì²´í¬
            if re.search(r'(ì„|ë¥¼|ì´|ê°€|ì€|ëŠ”|ì—|ì˜|ë¡œ|ì™€|ê³¼|ë„)', cleaned):
                continue

            # ë™ì‚¬ ì–´ë¯¸ ì²´í¬ (ì™„í™” - ê¸°ë³¸í˜•ë§Œ)
            if re.search(r'(í•˜ë‹¤|ë˜ë‹¤|í–ˆë‹¤|ëœë‹¤|ìˆë‹¤|í•˜ë©°|ë˜ë©°|í–ˆìœ¼ë©°)$', cleaned):
                continue

            # ê´€í˜•ì‚¬í˜• ì–´ë¯¸ (í•µì‹¬ë§Œ)
            if re.search(r'(í•œ|ëœ|í• |ë )$', cleaned):
                continue

            # ê¸¸ì´: 2-7ê¸€ì (ì™„í™”)
            if len(cleaned) < 2 or len(cleaned) > 7:
                continue

            # ë¶ˆìš©ì–´ ì²´í¬
            if cleaned.lower() in STOPWORDS:
                continue

            # ìœ íš¨ì„± ê²€ì‚¬
            if not is_valid_keyword(cleaned):
                continue

            merged.append(cleaned)
            seen.add(cleaned)

        # 4ï¸âƒ£ ë¶€ì¡±í•˜ë©´ KRWordRank ì¶”ê°€
        if len(merged) < 5:
            for kw in kr_kws:
                if len(merged) >= 5:
                    break

                cleaned = clean_keyword(kw)
                if not cleaned or len(cleaned) < 2 or cleaned in seen:
                    continue

                if " " in cleaned:
                    continue

                if re.search(r'(ì„|ë¥¼|ì´|ê°€|ì€|ëŠ”|ì—|ì˜|ë¡œ)', cleaned):
                    continue

                if re.search(r'(í•˜ë‹¤|ë˜ë‹¤|í–ˆë‹¤|í•œ|ëœ)$', cleaned):
                    continue

                if len(cleaned) > 7:
                    continue

                if cleaned.lower() in STOPWORDS:
                    continue

                if not is_valid_keyword(cleaned):
                    continue

                merged.append(cleaned)
                seen.add(cleaned)

        return merged if merged else ["í‚¤ì›Œë“œ ì—†ìŒ"]
    except Exception as e:
        print(f"[Error] Keyword extraction failed: {e}")
        return []

# ----------------------------------------------------
# ğŸ“° ì œëª© (ì§§ì€ ë‰´ìŠ¤ íƒ€ì´í‹€í˜•)
# ----------------------------------------------------
def generate_title(text: str, max_len: int = 25, min_len: int = 10) -> str:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ì œëª© ìƒì„± (ë‹¨ìˆœí•˜ê³  ë²”ìš©ì )"""

    # 1ï¸âƒ£ í‚¤ì›Œë“œ ì¶”ì¶œ (ìƒìœ„ 4ê°œ)
    keywords = extract_keywords(text)[:4]

    if not keywords or keywords == ["í‚¤ì›Œë“œ ì—†ìŒ"]:
        return "ì œëª© ì—†ìŒ"

    # 2ï¸âƒ£ í‚¤ì›Œë“œ ì¡°í•© (ê¸¸ì´ ì œí•œ ê³ ë ¤)
    title_parts = []
    current_length = 0

    for kw in keywords:
        # ê³µë°± í¬í•¨ ê¸¸ì´ ê³„ì‚°
        kw_len = len(kw) + (1 if title_parts else 0)  # ì•ì— ê³µë°± ì¶”ê°€

        if current_length + kw_len <= max_len:
            title_parts.append(kw)
            current_length += kw_len
        else:
            break  # ë” ì´ìƒ ì¶”ê°€í•˜ë©´ max_len ì´ˆê³¼

        if len(title_parts) >= 4:  # ìµœëŒ€ 4ê°œ
            break

    # 3ï¸âƒ£ ìµœì¢… ì œëª© ìƒì„±
    if not title_parts:
        # ìµœì•…ì˜ ê²½ìš° ì²« ë²ˆì§¸ í‚¤ì›Œë“œë¼ë„ ì‚¬ìš©
        title_parts = keywords[:1]

    title = " ".join(title_parts)

    # 4ï¸âƒ£ ìµœì¢… ê¸¸ì´ ì²´í¬
    if len(title) > max_len:
        # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì¶•ì•½
        words = title.split()
        title = " ".join(words[:3])

    return title.strip() if title else "ì œëª© ì—†ìŒ"
