# app/services.py
# ----------------------------------------------------
# ÏöîÏïΩ: Qwen2.5-1.5B-Instruct (CPU ÏµúÏ†ÅÌôî Î≤ÑÏ†Ñ)
# ÌÇ§ÏõåÎìú: KRWordRank + KeyBERT
# Ï†úÎ™©: Qwen + ÌõÑÏ≤òÎ¶¨
# ----------------------------------------------------

import re
import torch
from transformers import Qwen2Tokenizer
from transformers import AutoTokenizer, AutoModelForCausalLM
from krwordrank.word import KRWordRank
from keybert import KeyBERT

# ----------------------------------------------------
# LLM ÏÑ§Ï†ï
# ----------------------------------------------------
MODEL_NAME = "Qwen/Qwen2.5-Mini"
DEVICE = "cpu"

print(f"üìå Loading Qwen model: {MODEL_NAME} (CPU)")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    low_cpu_mem_usage=True,
    device_map=DEVICE,
)
model.eval()

print("üìå Loading KeyBERT...")
kw_model = KeyBERT(model="jhgan/ko-sroberta-multitask")
print("‚úÖ All models loaded.")


# ----------------------------------------------------
# ÌÖçÏä§Ìä∏ Í∏∏Ïù¥ Ï†úÌïú (Í∏∞Î≥∏ Í∞úÏÑ†Ìòï: 500Ïûê)
# ----------------------------------------------------
def shrink_text(text: str, limit: int = 500) -> str:
    text = text.strip()
    if len(text) <= limit:
        return text
    return text[:limit] + "..."


# ----------------------------------------------------
# 1) ÏöîÏïΩ ÏÉùÏÑ± (Í∏∞Î≥∏ Í∞úÏÑ†Ìòï)
# ----------------------------------------------------
def generate_summary(text: str, max_len: int = 150, min_len: int = 50) -> str:
    text = shrink_text(text, 500)

    prompt = f"""
ÏïÑÎûò Í∏ÄÏùÑ {min_len}~{max_len}ÏûêÏùò Ìïú Î¨∏Îã®ÏúºÎ°ú ÏûêÏó∞Ïä§ÎüΩÍ≤å ÏöîÏïΩÌïòÏÑ∏Ïöî.

‚ùó Í∏àÏßÄ Í∑úÏπô:
- Î¶¨Ïä§Ìä∏/Íµ¨Ï°∞Ìôî Í∏àÏßÄ
- ÎßàÌÅ¨Îã§Ïö¥(#, -, *, ‚Ä¢) Í∏àÏßÄ
- "Ï†ïÎ¶¨Ìï¥ ÎìúÎ¶ΩÎãàÎã§", "ÏöîÏïΩÌïòÎ©¥" Í∞ôÏùÄ ÌëúÌòÑ Í∏àÏßÄ
- Îëê Î¨∏Îã® Ïù¥ÏÉÅ Í∏àÏßÄ

Î≥∏Î¨∏:
{text}

ÏöîÏïΩ:
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=120,  # Í∏∞Î≥∏ Í∞úÏÑ†Ìòï
            do_sample=False,
        )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    summary = decoded.split("ÏöîÏïΩ:")[-1].strip()

    # ÌõÑÏ≤òÎ¶¨: ÎßàÌÅ¨Îã§Ïö¥, Î¶¨Ïä§Ìä∏ Ï†úÍ±∞
    summary = re.sub(r"[#*\-‚Ä¢].*", "", summary).strip()

    # "Î¨¥..." Í∞ôÏùÄ Ïù¥ÏÉÅÌïú ÎÅäÍπÄ Î∞©ÏßÄ
    summary = re.sub(r"[Í∞Ä-Ìû£]\Z", "", summary).strip()

    return summary


# ----------------------------------------------------
# 2) ÌÇ§ÏõåÎìú Ï∂îÏ∂ú (Í∏∞Î≥∏ Í∞úÏÑ†Ìòï)
# ----------------------------------------------------
STOPWORDS = {
    "Ïù¥Î≤à","Ï†ÄÎ≤à","Îã§Ïùå","Ïù¥","Í∑∏","Ï†Ä","Ïù¥Îü∞","Í∑∏Îü∞","Ï†ÄÎü∞",
    "Í≤É","Ïàò","Îì±","Ï†ê","Îïå","Í≥≥","Ï§ë","ÎÇ¥","Ïô∏",
    "Í∑∏Î¶¨Í≥†","Í∑∏Îü¨ÎÇò","ÌïòÏßÄÎßå","ÎòêÌïú","Îòê",
}

POSTFIX = ["Ïùò","ÏùÄ","Îäî","Ïù¥","Í∞Ä","ÏùÑ","Î•º","Ïóê","ÏóêÏÑú","ÏúºÎ°ú","Î°ú","Í≥º","ÏôÄ"]


def clean_keyword(word: str) -> str:
    w = word.strip()

    # Ï°∞ÏÇ¨ Ï†úÍ±∞
    for pf in POSTFIX:
        if w.endswith(pf) and len(w) > len(pf) + 1:
            w = w[: -len(pf)]

    w = re.sub(r"[^\wÍ∞Ä-Ìû£]", "", w)
    if re.fullmatch(r"[\W\d]+", w):
        return ""

    return w


def extract_keywords(text: str) -> list[str]:
    text = shrink_text(text, 800)

    sentences = [s.strip() for s in re.split(r"[.!?\n]+", text) if s.strip()]
    if not sentences:
        sentences = [text]

    # KRWordRank
    try:
        extractor = KRWordRank(min_count=2, max_length=10, verbose=False)
        scores, _, _ = extractor.extract(sentences, beta=0.85, max_iter=10)
        kw1 = list(scores.keys())
    except:
        kw1 = []

    # KeyBERT
    try:
        kw2_pairs = kw_model.extract_keywords(
            text, top_n=10, keyphrase_ngram_range=(1, 1),
            use_mmr=True, diversity=0.7
        )
        kw2 = [w for w, _ in kw2_pairs]
    except:
        kw2 = []

    # Î≥ëÌï© + ÌïÑÌÑ∞ÎßÅ
    merged = []
    seen = set()

    def add(w: str):
        w = clean_keyword(w)
        if not w:
            return
        if len(w) < 2:
            return
        if w in STOPWORDS:
            return
        if w in seen:
            return
        seen.add(w)
        merged.append(w)

    for w in kw2 + kw1:
        if len(merged) >= 5:
            break
        add(w)

    return merged or ["ÌÇ§ÏõåÎìú ÏóÜÏùå"]


# ----------------------------------------------------
# 3) Ï†úÎ™© ÏÉùÏÑ± (Í∏∞Î≥∏ Í∞úÏÑ†Ìòï)
# ----------------------------------------------------
def generate_title(text: str, max_len: int = 25, min_len: int = 10) -> str:
    text = shrink_text(text, 400)

    prompt = f"""
Îã§Ïùå Í∏ÄÏùò ÎÇ¥Ïö©ÏùÑ Í∏∞Î∞òÏúºÎ°ú Ìïú Ï§Ñ Ï†úÎ™©ÏùÑ ÎßåÎì§Îêò,
Ï†úÎ™© Ïô∏Ïùò Îã§Î•∏ Î¨∏Ïû•ÏùÄ Ï†àÎåÄ Ï∂úÎ†•ÌïòÏßÄ ÎßàÏÑ∏Ïöî.

Î≥∏Î¨∏:
{text}

Ï†úÎ™©:
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=60,
            do_sample=False
        )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    title = decoded.split("Ï†úÎ™©:")[-1].strip()

    # 1) Ïó¨Îü¨ Ï§Ñ ‚Üí Ï≤´ Ï§ÑÎßå
    title = title.splitlines()[0].strip()

    # 2) Îî∞Ïò¥Ìëú Ï†úÍ±∞
    title = title.strip('"').strip("'")

    # 3) ÎßàÌÅ¨Îã§Ïö¥ Í∏∞Ìò∏ Ï†úÍ±∞
    title = re.sub(r"[#*\-‚Ä¢`]", "", title).strip()

    # 4) ÎßàÏßÄÎßâ ÏùåÏ†à Ìïú Í∏ÄÏûê ÏûòÎ¶º Î≥¥Ï†ï
    if len(title) >= 2 and re.fullmatch(r"[Í∞Ä-Ìû£]", title[-1]):
        title = title[:-1].rstrip()

    # 5) Í∏∏Ïù¥ Ï†úÌïú
    if len(title) > max_len:
        title = title[:max_len].rstrip()
        if len(title) >= 2 and re.fullmatch(r"[Í∞Ä-Ìû£]", title[-1]):
            title = title[:-1].rstrip()

    # 6) ÏµúÏÜå Í∏∏Ïù¥ Î≥¥Ï†ï
    if len(title) < min_len:
        title = "ÏöîÏïΩ Ï†úÎ™©"

    return title
